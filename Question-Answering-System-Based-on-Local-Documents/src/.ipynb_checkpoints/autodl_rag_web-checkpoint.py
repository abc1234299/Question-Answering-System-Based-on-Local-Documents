# ä¾èµ–å®‰è£…å‘½ä»¤ï¼ˆç»ˆç«¯å…ˆæ‰§è¡Œï¼‰
# pip install langchain langchain_text_splitters faiss-gpu sentence-transformers ollama pypdf gradio -i https://pypi.tuna.tsinghua.edu.cn/simple

from langchain_text_splitters import RecursiveCharacterTextSplitter
# ä¿ç•™å…¶ä»–å¿…è¦å¯¼å…¥ï¼Œåªä¿®æ”¹è¿™ä¸¤è¡Œå…³äºåµŒå…¥å’Œå‘é‡å­˜å‚¨çš„
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import SentenceTransformerEmbeddings  # å…³é”®ï¼šå¯¼å…¥æ­£ç¡®çš„ç±»
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama
from langchain_community.document_loaders import PyPDFLoader, TextLoader
import gradio as gr
import os
import shutil

# RTX3090ä¸“å±é…ç½®
EMBED_MODEL = "m3e-large"  # ä¸­æ–‡GPUåµŒå…¥æ¨¡å‹ï¼ˆç²¾åº¦é«˜ã€é€Ÿåº¦å¿«ï¼‰
LLM_MODEL = "qwen2.5:7b"  # 7Bæ¨¡å‹ï¼ˆRTX3090è½»æ¾æ‰¿è½½ï¼‰
VECTOR_DB_PATH = "faiss_gpu_db"
UPLOAD_DIR = "uploaded_docs"  # ä¸Šä¼ æ–‡æ¡£ä¿å­˜ç›®å½•


# åˆå§‹åŒ–GPUç»„ä»¶
embeddings = SentenceTransformerEmbeddings(
    model_name="moka-ai/m3e-large",  # è®©ç¨‹åºè‡ªåŠ¨ä»å›½å†…æºä¸‹è½½
    model_kwargs={"device": "cuda"}
)
llm = Ollama(model=LLM_MODEL, num_gpu=1)  # åˆ†é…GPUç»™LLM

def build_or_load_db():
    # 1. å¯¼å…¥æ‰€éœ€ä¾èµ–ï¼ˆç¡®ä¿ä¸ç¼ºæ¨¡å—ï¼‰
    from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    global embeddings  # å¼•ç”¨å…¨å±€çš„ embeddings å˜é‡ï¼ˆä½ å·²åœ¨å‡½æ•°å¤–åˆå§‹åŒ–ï¼‰

    # 2. é…ç½®æ–‡æ¡£åŠ è½½å™¨ï¼ˆåŠ è½½ docs æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰ PDFï¼‰
    loader = DirectoryLoader(
        path="/root/autodl-tmp/docs",  # æ–‡æ¡£æ–‡ä»¶å¤¹ç»å¯¹è·¯å¾„ï¼ˆå’Œä½ åˆ›å»ºçš„ä¸€è‡´ï¼‰
        glob="*.pdf",  # ä»…åŠ è½½ PDF æ–‡ä»¶ï¼Œé¿å…æ— å…³æ–‡ä»¶å¹²æ‰°
        loader_cls=PyPDFLoader,  # æŒ‡å®š PDF è§£æå™¨
        show_progress=True  # æ˜¾ç¤ºåŠ è½½è¿›åº¦ï¼ˆå¯é€‰ï¼Œæ–¹ä¾¿æŸ¥çœ‹åŠ è½½çŠ¶æ€ï¼‰
    )

    # 3. åŠ è½½æ–‡æ¡£å¹¶éªŒè¯
    try:
        docs = loader.load()
        print(f"\nâœ… æˆåŠŸä» docs æ–‡ä»¶å¤¹åŠ è½½åˆ° {len(docs)} ä¸ª PDF æ–‡æ¡£")
    except Exception as e:
        print(f"\nâŒ æ–‡æ¡£åŠ è½½å¤±è´¥ï¼š{str(e)}")
        print("è¯·æ£€æŸ¥ï¼š1. docs æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨ 2. æ–‡ä»¶å¤¹å†…æ˜¯å¦æœ‰ PDF æ–‡ä»¶ 3. å·²å®‰è£… pypdf ä¾èµ–ï¼ˆpip install pypdfï¼‰")
        return None

    # 4. åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆè§£å†³ NameErrorï¼Œé€‚é…é•¿æ–‡æ¡£ï¼‰
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # æ¯ä¸ªç‰‡æ®µçš„å­—ç¬¦æ•°ï¼ˆå¯è°ƒæ•´ï¼Œå¦‚ 1500 é€‚åˆé•¿æ–‡æœ¬ï¼‰
        chunk_overlap=200,  # ç‰‡æ®µé—´é‡å å­—ç¬¦æ•°ï¼ˆé¿å…å‰²è£‚è¯­ä¹‰ï¼Œå»ºè®®ä¸º chunk_size çš„ 10%-20%ï¼‰
        length_function=len,  # æŒ‰å­—ç¬¦é•¿åº¦è®¡ç®—ï¼ˆä¸­æ–‡å‹å¥½ï¼‰
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""]  # ä¸­æ–‡ä¼˜å…ˆåˆ†å‰²ç¬¦
    )

    # 5. åˆ†å‰²æ–‡æ¡£å¹¶éªŒè¯
    split_docs = text_splitter.split_documents(docs)
    print(f"âœ… æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå¾—åˆ° {len(split_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

    # 6. æ„å»ºå¹¶ä¿å­˜ FAISS å‘é‡åº“
    if len(split_docs) > 0:
        try:
            db = FAISS.from_documents(split_docs, embeddings)
            db.save_local("faiss_db")  # ä¿å­˜åˆ°æœ¬åœ°ï¼Œä¸‹æ¬¡å¯ç›´æ¥åŠ è½½
            print(f"âœ… FAISS å‘é‡åº“åˆ›å»ºæˆåŠŸï¼å·²ä¿å­˜åˆ° faiss_db æ–‡ä»¶å¤¹")
            return db
        except Exception as e:
            print(f"\nâŒ å‘é‡åº“åˆ›å»ºå¤±è´¥ï¼š{str(e)}")
            return None
    else:
        print(f"\nâŒ æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£ç‰‡æ®µï¼ˆåˆ†å‰²åæ•°é‡ä¸º 0ï¼‰ï¼Œè¯·æ£€æŸ¥ PDF å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–è°ƒæ•´åˆ†å‰²å‚æ•°")
        return None
# ä¸Šä¼ æ–‡æ¡£å¹¶æ›´æ–°å‘é‡åº“
def upload_docs(files):
    if not files:
        return "æœªä¸Šä¼ ä»»ä½•æ–‡æ¡£ï¼", None
    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
    for file in files:
        shutil.copy(file, os.path.join(UPLOAD_DIR, os.path.basename(file)))
    # é‡å»ºå‘é‡åº“
    db = build_or_load_db()
    return f"âœ… æˆåŠŸä¸Šä¼  {len(files)} ä¸ªæ–‡æ¡£ï¼Œå·²æ›´æ–°çŸ¥è¯†åº“ï¼", db

# RAGé—®ç­”æ ¸å¿ƒå‡½æ•°
def rag_qa(query, db):
    if not db:
        return "âŒ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ï¼", ""
    retriever = db.as_retriever(search_kwargs={"k": 4})
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="map_reduce",
        retriever=retriever,
        return_source_documents=True
    )
    result = qa_chain({"query": query})
    
    # æ•´ç†å¼•ç”¨æ¥æº
    sources = ""
    for i, doc in enumerate(result["source_documents"][:3], 1):
        filename = os.path.basename(doc.metadata["source"])
        page = doc.metadata.get("page", 0) + 1  # PDFé¡µç ä»1å¼€å§‹
        content = doc.page_content[:180] + "..." if len(doc.page_content) > 180 else doc.page_content
        sources += f"\nã€å¼•ç”¨{i}ã€‘ã€Š{filename}ã€‹ç¬¬{page}é¡µï¼š{content}"
    
    return result["result"], f"ğŸ” å‚è€ƒæ¥æºï¼š{sources}"

# æ„å»ºGradio Webç•Œé¢
with gr.Blocks(title="RTX3090 RAGçŸ¥è¯†åº“") as demo:
    gr.Markdown("# ğŸ“š æœ¬åœ°çŸ¥è¯†åº“é—®ç­”ï¼ˆRTX3090åŠ é€Ÿï¼‰")
    gr.Markdown("æ”¯æŒä¸Šä¼ PDF/TXTæ–‡æ¡£ï¼ŒåŸºäºæ–‡æ¡£å†…å®¹ç²¾å‡†é—®ç­”ï¼ˆå¸¦å¼•ç”¨æº¯æºï¼‰")
    
    # å­˜å‚¨å‘é‡åº“å®ä¾‹ï¼ˆå…¨å±€å˜é‡ï¼‰
    db_state = gr.State(None)
    
    with gr.Row():
        with gr.Column(scale=1):
            file_upload = gr.File(
                label="ä¸Šä¼ æ–‡æ¡£ï¼ˆæ”¯æŒå¤šæ–‡ä»¶ï¼‰",
                file_types=[".pdf", ".txt"],
                file_count="multiple"
            )
            upload_btn = gr.Button("ğŸ“¤ ä¸Šä¼ å¹¶æ›´æ–°çŸ¥è¯†åº“")
            upload_status = gr.Textbox(label="ä¸Šä¼ çŠ¶æ€", interactive=False)
            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºçŸ¥è¯†åº“")
        
        with gr.Column(scale=2):
            query_input = gr.Textbox(label="è¯·æé—®", placeholder="ä¾‹å¦‚ï¼šæ–‡æ¡£ä¸­æåˆ°çš„æ ¸å¿ƒç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿ")
            qa_btn = gr.Button("ğŸš€ å¼€å§‹é—®ç­”")
            answer_output = gr.Textbox(label="å›ç­”", lines=8, interactive=False)
            source_output = gr.Textbox(label="å¼•ç”¨æ¥æº", lines=4, interactive=False)
    
    # ç»‘å®šæŒ‰é’®äº‹ä»¶
    upload_btn.click(
        fn=upload_docs,
        inputs=file_upload,
        outputs=[upload_status, db_state]
    )
    
    qa_btn.click(
        fn=rag_qa,
        inputs=[query_input, db_state],
        outputs=[answer_output, source_output]
    )
    
    # æ¸…ç©ºçŸ¥è¯†åº“
    def clear_knowledge():
        if os.path.exists(VECTOR_DB_PATH):
            shutil.rmtree(VECTOR_DB_PATH)
        if os.path.exists(UPLOAD_DIR):
            shutil.rmtree(UPLOAD_DIR)
            os.makedirs(UPLOAD_DIR)
        return "âœ… çŸ¥è¯†åº“å·²æ¸…ç©ºï¼", None
    clear_btn.click(fn=clear_knowledge, outputs=[upload_status, db_state])

# å¯åŠ¨WebæœåŠ¡ï¼ˆé€‚é…AutoDLç«¯å£ï¼‰
if __name__ == "__main__":
    # åˆå§‹åŒ–å‘é‡åº“ï¼ˆé¦–æ¬¡è¿è¡Œä¸ºç©ºï¼‰
    build_or_load_db()
    demo.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,        # AutoDLé»˜è®¤è‡ªå®šä¹‰ç«¯å£
        show_error=True,
        share=False  # AutoDLæ— éœ€é¢å¤–åˆ†äº«ï¼Œç›´æ¥ç”¨å®ä¾‹é“¾æ¥è®¿é—®
    )