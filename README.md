# Question-Answering-System-Based-on-Local-Documents
# ğŸ“š åŸºäºæœ¬åœ°æ–‡æ¡£çš„ RAG é—®ç­”ç³»ç»Ÿï¼ˆRTX3090 åŠ é€Ÿç‰ˆï¼‰

ä¸€ä¸ªåŸºäºæœ¬åœ°æ–‡æ¡£çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é—®ç­”ç³»ç»Ÿï¼Œåˆ©ç”¨ RTX3090 GPU åŠ é€Ÿï¼Œæ”¯æŒ PDF/TXT æ–‡æ¡£ä¸Šä¼ ã€æ™ºèƒ½é—®ç­”ä¸å¼•ç”¨æº¯æºã€‚

---

## âœ¨ åŠŸèƒ½ç‰¹ç‚¹

- ğŸš€ **GPU åŠ é€Ÿ**ï¼šä¸“ä¸º RTX3090 ä¼˜åŒ–ï¼Œä½¿ç”¨ `m3e-large` ä¸­æ–‡åµŒå…¥æ¨¡å‹å’Œ `qwen2.5:7b` LLM æ¨¡å‹
- ğŸ“„ **å¤šæ ¼å¼æ”¯æŒ**ï¼šæ”¯æŒ PDF å’Œ TXT æ ¼å¼æ–‡æ¡£ä¸Šä¼ ä¸è§£æ
- ğŸ” **æ™ºèƒ½æ£€ç´¢**ï¼šåŸºäº FAISS å‘é‡æ•°æ®åº“å®ç°è¯­ä¹‰æ£€ç´¢
- ğŸ“– **å¼•ç”¨æº¯æº**ï¼šå›ç­”é™„å¸¦åŸæ–‡å¼•ç”¨ï¼Œæ”¯æŒé¡µç å’Œå†…å®¹ç‰‡æ®µå±•ç¤º
- ğŸŒ **Web ç•Œé¢**ï¼šåŸºäº Gradio æ„å»ºå‹å¥½çš„äº¤äº’ç•Œé¢
- ğŸ“‚ **æ–‡æ¡£ç®¡ç†**ï¼šæ”¯æŒæ‰¹é‡ä¸Šä¼ ã€æ¸…ç©ºçŸ¥è¯†åº“ç­‰æ“ä½œ

---

## ğŸ“ é¡¹ç›®ç»“æ„

| ç›®å½•/æ–‡ä»¶ | è¯´æ˜ |
|----------|------|
| **docs/** | é»˜è®¤æ–‡æ¡£å­˜æ”¾ç›®å½•ï¼ˆå­˜æ”¾åˆå§‹PDFæ–‡æ¡£ï¼‰ |
| **uploaded_docs/** | ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£ä¿å­˜ç›®å½• |
| **faiss_gpu_db/** | FAISSå‘é‡æ•°æ®åº“ï¼ˆè¿è¡Œåè‡ªåŠ¨ç”Ÿæˆï¼‰ |
| **models/** | æ¨¡å‹æ–‡ä»¶ç›®å½•ï¼ˆå¦‚éœ€è¦æœ¬åœ°å­˜å‚¨æ¨¡å‹ï¼‰ |
| **results/** | è¿è¡Œç»“æœå’Œè¾“å‡ºæ–‡ä»¶ |
| **scripts/** | è¾…åŠ©è„šæœ¬å’Œå·¥å…· |
| **src/** | æºä»£ç ç›®å½•ï¼ˆåŒ…å«ä¸»ç¨‹åºï¼‰ |
| **LICENSE** | é¡¹ç›®è®¸å¯è¯æ–‡ä»¶ |
| **README.md** | é¡¹ç›®è¯´æ˜æ–‡æ¡£ï¼ˆæœ¬æ–‡æ¡£ï¼‰ |
| **requirement.txt** | Pythonä¾èµ–åŒ…åˆ—è¡¨ |
| **ollama.log** | Ollamaæ¨¡å‹æœåŠ¡æ—¥å¿— |

## ğŸ”§ å®‰è£…æ­¥éª¤
```markdown
### 1. å…‹éš†é¡¹ç›®
```
git clone https://github.com/abc1234299/Question-Answering-System-Based-on-Local-Documents.git
cd Question-Answering-System-Based-on-Local-Documents

### 2. å®‰è£… Python ä¾èµ–
```markdown
pip install langchain langchain_text_splitters faiss-gpu sentence-transformers ollama pypdf gradio -i https://pypi.tuna.tsinghua.edu.cn/simple

### 3. éªŒè¯å®‰è£…
python -c "import langchain; print('âœ… LangChain å®‰è£…æˆåŠŸ')"

### 4. å®‰è£… Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen2.5:7b
```

## ğŸš€ å¿«é€Ÿä½¿ç”¨
```markdown
### å¯åŠ¨ç³»ç»Ÿ
```
python src/main.py

### è®¿é—® Web ç•Œé¢
http://<æœåŠ¡å™¨IP>:7860


## âš¡ æ€§èƒ½ä¼˜åŒ–é…ç½®
é’ˆå¯¹ RTX3090 æ˜¾å¡ç‰¹æ€§ï¼Œä»¥ä¸‹é…ç½®å¯æœ€å¤§åŒ–åˆ©ç”¨ç¡¬ä»¶æ€§èƒ½ï¼Œå…¼é¡¾é—®ç­”é€Ÿåº¦å’Œå‡†ç¡®ç‡ï¼š

### 1. GPU å…¨é“¾è·¯åŠ é€Ÿè®¾ç½®
```python
# åµŒå…¥æ¨¡å‹ GPU åŠ é€Ÿï¼ˆm3e-large ä¸­æ–‡æ¨¡å‹ï¼‰
embeddings = SentenceTransformerEmbeddings(
    model_name="moka-ai/m3e-large",
    model_kwargs={"device": "cuda"}  # å¼ºåˆ¶ä½¿ç”¨ GPU è¿›è¡ŒåµŒå…¥è®¡ç®—
)

# LLM æ¨¡å‹ GPU åˆ†é…ï¼ˆqwen2.5:7bï¼‰
llm = Ollama(model="qwen2.5:7b", num_gpu=1)  # num_gpu=1 ä¸ºæ¨¡å‹åˆ†é…å…¨éƒ¨ GPU èµ„æº
```
### 2. æ˜¾å­˜ä¼˜åŒ–
# æ–¹æ¡ˆ1ï¼šè°ƒæ•´æ–‡æœ¬åˆ†å‰²å‚æ•°ï¼ˆå‡å°‘å•æ¬¡è®¡ç®—æ˜¾å­˜å ç”¨ï¼‰
```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1200,  # é€‚åº¦å¢å¤§ chunk å°ºå¯¸ï¼Œå‡å°‘æ€»ç‰‡æ®µæ•°
    chunk_overlap=150,  # é™ä½é‡å ç‡ï¼Œå‡å°‘é‡å¤è®¡ç®—
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ"]
)
```
# æ–¹æ¡ˆ2ï¼šLLM æ˜¾å­˜é™åˆ¶ï¼ˆé¿å…æ˜¾å­˜æº¢å‡ºï¼‰
```python
llm = Ollama(
    model="qwen2.5:7b",
    num_gpu=1,
    num_ctx=8192,  # ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆå¹³è¡¡æ˜¾å­˜å’Œé—®ç­”èƒ½åŠ›ï¼‰
    temperature=0.1  # é™ä½éšæœºæ€§ï¼Œå‡å°‘è®¡ç®—é‡
)
```
# æ–¹æ¡ˆ3ï¼šFAISS GPU ç´¢å¼•ä¼˜åŒ–
```python
db = FAISS.from_documents(split_docs, embeddings)
db = db.to_gpu()  # å¼ºåˆ¶å°†å‘é‡åº“åŠ è½½åˆ° GPUï¼Œæ£€ç´¢é€Ÿåº¦æå‡ 5-10 å€
```

## ğŸ› å¸¸è§é—®é¢˜
### Q1ï¼šä¾èµ–å®‰è£…å¤±è´¥
**é—®é¢˜ç°è±¡**ï¼šæ‰§è¡Œ `pip install` æ—¶å‡ºç°åŒ…å†²çªã€ç¼–è¯‘å¤±è´¥æˆ–ä¸‹è½½è¶…æ—¶  
**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ–¹æ¡ˆ1ï¼šä½¿ç”¨è™šæ‹Ÿç¯å¢ƒéš”ç¦»ä¾èµ–ï¼ˆæ¨èï¼‰
python -m venv rag_venv
source rag_venv/bin/activate  # Linux/Mac
# rag_venv\Scripts\activate  # Windows

# æ–¹æ¡ˆ2ï¼šåˆ†æ­¥å®‰è£…ï¼Œä¼˜å…ˆè§£å†³ faiss-gpu ä¾èµ–é—®é¢˜
pip install faiss-gpu==1.7.2 --no-deps -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install langchain sentence-transformers pypdf gradio ollama -i https://pypi.tuna.tsinghua.edu.cn/simple

# æ–¹æ¡ˆ3ï¼šé™çº§ pip ç‰ˆæœ¬ï¼ˆé€‚é…éƒ¨åˆ†ç³»ç»Ÿï¼‰
pip install pip==23.0.1
```
### Q2ï¼šOllama æ¨¡å‹ä¸‹è½½æ…¢ / å¤±è´¥
**é—®é¢˜ç°è±¡**ï¼šollama pull qwen2.5:7b é€Ÿåº¦ææ…¢æˆ–æç¤ºè¿æ¥è¶…æ—¶è§£å†³æ–¹æ¡ˆï¼š
**è§£å†³æ–¹æ¡ˆ**
```bash
# ä¸´æ—¶é…ç½®é•œåƒæºï¼ˆå•æ¬¡ç”Ÿæ•ˆï¼‰
export OLLAMA_HOST=https://mirror.ghproxy.com
ollama pull qwen2.5:7b

# æ°¸ä¹…é…ç½®é•œåƒæºï¼ˆLinuxï¼‰
echo 'export OLLAMA_HOST=https://mirror.ghproxy.com' >> ~/.bashrc
source ~/.bashrc
ollama pull qwen2.5:7b

# å¤‡é€‰æ–¹æ¡ˆï¼šæ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶åå¯¼å…¥
# 1. ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°æœ¬åœ°
# 2. æ‰§è¡Œï¼šollama create qwen2.5:7b -f ./Modelfile
```
### Q3ï¼šGPU å†…å­˜ä¸è¶³ï¼ˆOOM æŠ¥é”™
**é—®é¢˜ç°è±¡**ï¼šè¿è¡Œæ—¶æç¤º CUDA out of memory æˆ–ç¨‹åºå´©æºƒè§£å†³æ–¹æ¡ˆï¼š
**è§£å†³æ–¹æ¡ˆ**
```python
# æ–¹æ¡ˆ1ï¼šå‡å°‘æ£€ç´¢ç‰‡æ®µæ•°é‡ï¼ˆé™ä½ LLM æ¨ç†å‹åŠ›ï¼‰
retriever = db.as_retriever(search_kwargs={"k": 2})  # ä» 4 é™è‡³ 2

# æ–¹æ¡ˆ2ï¼šæ›´æ¢è½»é‡åµŒå…¥æ¨¡å‹ + è°ƒæ•´æ‰¹å¤„ç†å¤§å°
embeddings = SentenceTransformerEmbeddings(
    model_name="moka-ai/m3e-base",  # æ›¿æ¢ä¸ºåŸºç¡€ç‰ˆï¼ˆæ˜¾å­˜å ç”¨å‡å°‘ 50%ï¼‰
    model_kwargs={"device": "cuda", "batch_size": 16}  # é™ä½æ‰¹å¤„ç†å¤§å°
)

# æ–¹æ¡ˆ3ï¼šä½¿ç”¨æ›´å°çš„ LLM æ¨¡å‹
llm = Ollama(model="qwen2.5:4b", num_gpu=1)  # 4B æ¨¡å‹æ›¿ä»£ 7B æ¨¡å‹

# æ–¹æ¡ˆ4ï¼šå¯ç”¨æ˜¾å­˜åˆ†ç‰‡ï¼ˆç»ˆææ–¹æ¡ˆï¼‰
import torch
torch.cuda.empty_cache()  # æ¸…ç†æ˜¾å­˜ç¼“å­˜
embeddings = SentenceTransformerEmbeddings(
    model_name="moka-ai/m3e-large",
    model_kwargs={"device": "cuda", "trust_remote_code": True, "load_in_8bit": True}
)
```
